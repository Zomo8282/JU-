{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYBCEq0mdxkq",
        "outputId": "2cc86a70-6fed-473c-a8e0-49d24157052b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arabic Text:\n",
            "\n",
            "20 Most Common Unigrams:\n",
            "[(('حم',), 7), (('الم',), 6), (('الر',), 5), (('طسم',), 2), (('المص',), 1), (('المر',), 1), (('كهيعص',), 1), (('طه',), 1), (('طس',), 1), (('يس',), 1), (('ص',), 1), (('عسق',), 1), (('ق',), 1), (('ن',), 1)]\n",
            "\n",
            "20 Most Common Bigrams:\n",
            "[(('حم', 'حم'), 5), (('الم', 'الم'), 4), (('الر', 'الر'), 3), (('الم', 'المص'), 1), (('المص', 'الر'), 1), (('الر', 'المر'), 1), (('المر', 'الر'), 1), (('الر', 'كهيعص'), 1), (('كهيعص', 'طه'), 1), (('طه', 'طسم'), 1), (('طسم', 'طس'), 1), (('طس', 'طسم'), 1), (('طسم', 'الم'), 1), (('الم', 'يس'), 1), (('يس', 'ص'), 1), (('ص', 'حم'), 1), (('حم', 'عسق'), 1), (('عسق', 'حم'), 1), (('حم', 'ق'), 1), (('ق', 'ن'), 1)]\n",
            "\n",
            "20 Most Common Trigrams:\n",
            "[(('حم', 'حم', 'حم'), 3), (('الم', 'الم', 'الم'), 2), (('الم', 'الم', 'المص'), 1), (('الم', 'المص', 'الر'), 1), (('المص', 'الر', 'الر'), 1), (('الر', 'الر', 'الر'), 1), (('الر', 'الر', 'المر'), 1), (('الر', 'المر', 'الر'), 1), (('المر', 'الر', 'الر'), 1), (('الر', 'الر', 'كهيعص'), 1), (('الر', 'كهيعص', 'طه'), 1), (('كهيعص', 'طه', 'طسم'), 1), (('طه', 'طسم', 'طس'), 1), (('طسم', 'طس', 'طسم'), 1), (('طس', 'طسم', 'الم'), 1), (('طسم', 'الم', 'الم'), 1), (('الم', 'الم', 'يس'), 1), (('الم', 'يس', 'ص'), 1), (('يس', 'ص', 'حم'), 1), (('ص', 'حم', 'حم'), 1)]\n",
            "\n",
            "------------------------------------------------------\n",
            "\n",
            "English Text:\n",
            "\n",
            "20 Most Common Unigrams:\n",
            "[(('the',), 7379), (('and',), 5546), (('of',), 4838), (('is',), 3095), (('Allah',), 2914), (('they',), 2527), (('that',), 2354), (('And',), 2325), (('them',), 2320), (('a',), 2189), (('in',), 2095), (('to',), 2065), (('for',), 1896), (('not',), 1839), (('unto',), 1786), (('ye',), 1765), (('who',), 1754), (('will',), 1616), (('you',), 1521), (('He',), 1451)]\n",
            "\n",
            "20 Most Common Bigrams:\n",
            "[(('of', 'the'), 1097), (('those', 'who'), 782), (('in', 'the'), 696), (('and', 'the'), 601), (('that', 'which'), 542), (('Allah', 'is'), 455), (('will', 'be'), 420), (('of', 'Allah'), 417), (('is', 'the'), 392), (('the', 'earth'), 371), (('it', 'is'), 321), (('Allah', 'and'), 318), (('unto', 'them'), 313), (('that', 'they'), 270), (('of', 'them'), 261), (('who', 'believe'), 256), (('that', 'ye'), 242), (('He', 'will'), 235), (('from', 'the'), 233), (('they', 'are'), 233)]\n",
            "\n",
            "20 Most Common Trigrams:\n",
            "[(('the', 'heavens', 'and'), 167), (('and', 'the', 'earth'), 148), (('heavens', 'and', 'the'), 135), (('of', 'those', 'who'), 126), (('those', 'who', 'believe'), 122), (('in', 'the', 'earth'), 122), (('those', 'who', 'disbelieve'), 115), (('is', 'in', 'the'), 108), (('for', 'those', 'who'), 107), (('Lo', 'Allah', 'is'), 106), (('of', 'that', 'which'), 101), (('the', 'Day', 'of'), 99), (('He', 'is', 'the'), 98), (('that', 'which', 'they'), 95), (('whom', 'He', 'will'), 92), (('O', 'ye', 'who'), 91), (('the', 'earth', 'and'), 91), (('that', 'which', 'is'), 90), (('Allah', 'and', 'His'), 88), (('ye', 'who', 'believe'), 88)]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import FreqDist, ngrams\n",
        "\n",
        "# Load Quran text in Arabic\n",
        "with open('A.txt', 'r', encoding='utf-8') as file:\n",
        "    quran_arabic = file.read()\n",
        "\n",
        "# Load Quran translation in English\n",
        "with open('E.txt', 'r', encoding='utf-8') as file:\n",
        "    quran_english = file.read()\n",
        "\n",
        "# Tokenize the text and remove any non-alphabetic tokens\n",
        "tokens_arabic = [word for word in nltk.word_tokenize(quran_arabic) if word.isalpha()]\n",
        "tokens_english = [word for word in nltk.word_tokenize(quran_english) if word.isalpha()]\n",
        "\n",
        "# Compute unigrams, bigrams, and trigrams for Arabic and English text\n",
        "unigrams_arabic = ngrams(tokens_arabic, 1)\n",
        "bigrams_arabic = ngrams(tokens_arabic, 2)\n",
        "trigrams_arabic = ngrams(tokens_arabic, 3)\n",
        "\n",
        "unigrams_english = ngrams(tokens_english, 1)\n",
        "bigrams_english = ngrams(tokens_english, 2)\n",
        "trigrams_english = ngrams(tokens_english, 3)\n",
        "\n",
        "# Compute frequency distribution of unigrams, bigrams, and trigrams for Arabic and English text\n",
        "freq_dist_unigrams_arabic = FreqDist(unigrams_arabic)\n",
        "freq_dist_bigrams_arabic = FreqDist(bigrams_arabic)\n",
        "freq_dist_trigrams_arabic = FreqDist(trigrams_arabic)\n",
        "\n",
        "freq_dist_unigrams_english = FreqDist(unigrams_english)\n",
        "freq_dist_bigrams_english = FreqDist(bigrams_english)\n",
        "freq_dist_trigrams_english = FreqDist(trigrams_english)\n",
        "\n",
        "# Print the 20 most common unigrams, bigrams, and trigrams for Arabic and English text\n",
        "print(\"Arabic Text:\\n\")\n",
        "print(\"20 Most Common Unigrams:\")\n",
        "print(freq_dist_unigrams_arabic.most_common(20))\n",
        "print(\"\\n20 Most Common Bigrams:\")\n",
        "print(freq_dist_bigrams_arabic.most_common(20))\n",
        "print(\"\\n20 Most Common Trigrams:\")\n",
        "print(freq_dist_trigrams_arabic.most_common(20))\n",
        "\n",
        "print(\"\\n------------------------------------------------------\\n\")\n",
        "\n",
        "print(\"English Text:\\n\")\n",
        "print(\"20 Most Common Unigrams:\")\n",
        "print(freq_dist_unigrams_english.most_common(20))\n",
        "print(\"\\n20 Most Common Bigrams:\")\n",
        "print(freq_dist_bigrams_english.most_common(20))\n",
        "print(\"\\n20 Most Common Trigrams:\")\n",
        "print(freq_dist_trigrams_english.most_common(20))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3BesPtcfx9p",
        "outputId": "deda4b58-b785-4470-a505-9832635dd7db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMb2U5z4exT5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}